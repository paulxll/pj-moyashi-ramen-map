{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ラーメンデータベースの店舗一覧ページをクローリングする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 「二郎系」タグのページ数を目視で確認したところ、1ページ目から71ページ目まで存在することを確認したので、順番にクローリングしていく\n",
    "for n in range(1,72):\n",
    "    url = 'https://ramendb.supleks.jp/search?order=point&type=0&station-id=0&tags=3&page={}'.format(n)\n",
    "    result = requests.get(url)\n",
    "    c = result.content\n",
    "    # ローカルディレクトリにクローリング結果をそのまま格納していく\n",
    "    with open('htmls/jiro_{}.html'.format(n),'w') as f:\n",
    "        f.write(c.decode('UTF-8'))\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スクレイピング1\n",
    "* 店舗一覧ページの中から、店舗名、レビュースコア、レビュー数、営業ステータス( 営業中・移転・閉店)、店舗URLを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(['htmls/'+ x for x in os.listdir('htmls/') if re.search('html$',x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上記のデータの中にはすでに移転した店舗や閉店済みの店舗も含まれるため、それらを区別する必要がある。\n",
    "def check_business_status(s):\n",
    "    \"\"\"\n",
    "    営業中・移転・閉店を判定する関数\n",
    "    \"\"\"\n",
    "    if s.find('span',{'class':'status_plate moved'}):\n",
    "        ret = \"移転\"\n",
    "    elif s.find('span',{'class':'status_plate retire'}):\n",
    "        ret = \"閉店\"\n",
    "    else:\n",
    "        ret = \"営業\"\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクレイピング結果をpandas dataframeに格納していく\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    with open(file,'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), 'lxml')\n",
    "    result_dic = dict()\n",
    "    summary = soup.find('div', {'class':'wrap'})\n",
    "    for ind, s in enumerate(summary.find_all('li',{'class':'border-box'})):\n",
    "        result_dic[ind] = dict()\n",
    "        result_dic[ind]['name'] = s.find('div',{'class':'name'}).find('h4').text\n",
    "        result_dic[ind]['pref'] = s.find('div',{'class':'area'}).find('a').text\n",
    "        result_dic[ind]['review_score'] = s.find('div',{'class':'point-val'}).text\n",
    "        result_dic[ind]['review_num'] = s.find('div',{'class':'val'}).text\n",
    "        result_dic[ind]['status'] = check_business_status(s)\n",
    "        result_dic[ind]['url'] = 'https://ramendb.supleks.jp' + s.find('a',{'class':'bglink'}).get('href')\n",
    "    tmp_df = pd.DataFrame.from_dict(result_dic, orient='index')\n",
    "    df = pd.concat([df,tmp_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.status == \"営業\"].pref.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クローリング2\n",
    "\n",
    "* 各店舗の住所を取得するために、各店舗のURLをクローリングしてhtmlを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in tqdm(df[df['status']=='営業'].itertuples()):\n",
    "    name = row.name\n",
    "    url = row.url\n",
    "    result = requests.get(url)\n",
    "    c = result.content\n",
    "    with open('htmls/shop_detail/{}.html'.format(name.replace(\"/\",\"\")),'w') as f:\n",
    "        f.write(c.decode('UTF-8'))\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### スクレイピング2\n",
    "\n",
    "* 各店舗のhtmlファイルから住所を取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(['htmls/shops/'+ x for x in os.listdir('htmls/shops/')])\n",
    "file = files[0]\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dic = {}\n",
    "for file in files:\n",
    "    with open(file,'r') as f:\n",
    "        soup = BeautifulSoup(f.read(), 'lxml')\n",
    "    url = 'https://ramendb.supleks.jp' + soup.find('h1').find('a').get('href')\n",
    "    result_dic[url] = soup.find('div',{'class':'datas'}).find('span',{'itemprop':'address'}).text.split('このお店は')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果をpandas dataframeに格納していく\n",
    "address_df = pd.DataFrame.from_dict(result_dic, orient='index').reset_index()\n",
    "address_df.columns = ['url', 'address']\n",
    "address_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先ほど作成したdataframeとマージする\n",
    "active_df = df[df.status == '営業'].reset_index(drop=True)\n",
    "active_df = active_df.merge(address_df)\n",
    "active_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_df.to_csv(\"active_shops.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate(address: str, dest_url: str='http://www.geocoding.jp/api/') -> tuple(str, str):\n",
    "    \"\"\"\n",
    "    addressに住所を指定すると緯度経度を返す。\n",
    "\n",
    "    >>> coordinate('東京都文京区本郷7-3-1')\n",
    "    ['35.712056', '139.762775']\n",
    "    \"\"\"\n",
    "    payload = {'q': address}\n",
    "    html = requests.get(dest_url, params=payload)\n",
    "    soup = BeautifulSoup(html.content, \"html.parser\")\n",
    "    if not soup.find('lat'):\n",
    "        print(f\"Invalid address submitted. {address}\")\n",
    "        return ('0', '0')\n",
    "    latitude = soup.find('lat').string\n",
    "    longitude = soup.find('lng').string\n",
    "    return (latitude, longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addresses = active_df.address.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('address_lat_lon.tsv', 'w') as f:\n",
    "    for address in tqdm(addresses):\n",
    "        lat, lon = coordinate(address)\n",
    "        f.write(f\"{address}\\t{lat}\\t{lon}\\n\")\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "address_df = pd.read_csv(\"address_lat_lon.tsv\", sep='\\t',names=[\"address\",\"lat\",\"lon\"])\n",
    "print(address_df.shape)\n",
    "address_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_address(ad):\n",
    "    \"\"\"\n",
    "    郵便番号、それから住所末尾のビル名を取り除く関数\n",
    "    \"\"\"\n",
    "    ad = re.sub(\"〒[0-9]{3}\\-[0-9]{4} \",\"\",ad)\n",
    "    ad = ad.split(\" \")[0]\n",
    "    return ad\n",
    "\n",
    "# 緯度経度取得に失敗した地点の住所をクレンジングする\n",
    "fail_df = address_df.query(\"lat == 0\")\n",
    "fail_df.loc[:,\"cleansed_address\"] = fail_df.address.apply(lambda x:clean_address(x))\n",
    "fail_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再度APIに投げる\n",
    "address_fail = fail_df.cleansed_address\n",
    "\n",
    "with open('address_lat_lon_fail.tsv', 'w') as f:\n",
    "    for address in tqdm(address_fail):\n",
    "        lat, lon = coordinate(address)\n",
    "        f.write(f\"{address}\\t{lat}\\t{lon}\\n\")\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 細かいデータ整形\n",
    "address_df = pd.read_csv(\"address_lat_lon.tsv\", sep='\\t',names=[\"address\",\"lat\",\"lon\"])\n",
    "print(address_df.shape)\n",
    "display(address_df.head())\n",
    "\n",
    "fail_df = address_df[address_df.lat == 0]\n",
    "fail_df.loc[:,\"cleansed_address\"] = fail_df.address.apply(lambda x:clean_address(x))\n",
    "print(fail_df.shape)\n",
    "display(fail_df.head())\n",
    "fail_result_df = pd.read_csv(\"address_lat_lon_fail.tsv\", sep=\"\\t\",names=[\"cleansed_address\",\"lat\",\"lon\"])\n",
    "fail_df = fail_df[[\"address\",\"cleansed_address\"]].merge(fail_result_df,on=\"cleansed_address\")[[\"address\",\"lat\",\"lon\"]]\n",
    "print(fail_df.shape)\n",
    "display(fail_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_address_df = pd.concat([address_df[address_df.lat !=0],fail_df[fail_df.lat !=0]])\n",
    "print(merged_address_df.shape)\n",
    "display(merged_address_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_df = pd.read_csv(\"active_shops.csv\")\n",
    "print(active_df.shape)\n",
    "display(active_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_df = active_df.merge(merged_address_df, on=\"address\", how=\"inner\")\n",
    "active_df.to_csv(\"active_shops_with_latlon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.1 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "e20b3dc21e5b7ceedb80d74a1953a41e6dbcb585d2207edf280bfb5080b51278"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
